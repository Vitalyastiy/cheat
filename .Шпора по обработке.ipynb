{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPM4kGGQoMuSupr4aqM5TJ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vitalyastiy/cheat-python/blob/main/.%D0%A8%D0%BF%D0%BE%D1%80%D0%B0%20%D0%BF%D0%BE%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импорт библиотек"
      ],
      "metadata": {
        "id": "kcNCXOIMOQQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import count\n",
        "from multiprocessing import cpu_count\n",
        "data = sns.load_dataset(\"titanic\")\n",
        "data_set1 = sns.load_dataset(\"taxis\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9rLcmvJS3PBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# тест код\n"
      ],
      "metadata": {
        "id": "1qolRx9kGwPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GhwwK5g9Wqqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Загрузка и выгрузка данных во фрейм"
      ],
      "metadata": {
        "id": "j0HjzJ66Iu60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv('тест.csv', parse_dates=['date'],  dayfirst=True) # автоконвертация текста в даты\n",
        "df = pd.read_csv('тест.csv', parse_dates=[['год','месяц','день']]) # «год», «месяц», «день», то при импорте данных их можно объединить, передав через параметр «parse_dates» список списка наименований столбцов\n",
        "\n",
        "df = pd.read_csv(r\"C:\\Users\\....Desktop\\nnn\\NN.csv\",  sep=',' , encoding='latin-1') #, index_col=0 - индекс\n",
        "after = before.encode(\"utf-8\", errors=\"replace\")# конвертировать столбец\n",
        "print(after.decode(\"utf-8\"))# показать\n",
        "\n",
        "df[:10].to_csv('saved_ratings.csv', index=False) # экспортироватъ первые десятъ строк\n",
        "data_copy = df.copy(deep=True) #shallow - поверхностное копирование фрейма\n",
        "df.to_csv('02-03_new.csv', header=True, index=False) # экспорт в csv\n",
        "pd.read_csv('file.csv', usecols=['A', 'B']) # - считать только нужные столбцы\n",
        "\n",
        "#определить тип кодировки\n",
        "import charset_normalizer\n",
        "# look at the first ten thousand bytes to guess the character encoding\n",
        "with open('train.csv', 'rb') as rawdata:\n",
        "    result = charset_normalizer.detect(rawdata.read(10000))\n",
        "# check what the character encoding might be\n",
        "print(result)\n",
        "#изменить кодировку на ютф\n",
        "before = sample_entry.decode(\"big5-tw\")\n",
        "new_entry = before.encode()"
      ],
      "metadata": {
        "id": "zk_98fsmI5kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Регулирование количества столбцов или строк для отображения"
      ],
      "metadata": {
        "id": "UT7oRCq1wT4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 10)  #Сброс ограничений на количество выводимых рядов\n",
        "pd.set_option('display.max_columns', 10)  # Сброс ограничений на число столбцов\n",
        "pd.set_option('display.max_colwidth', 10)   # Сброс ограничений на количество символов в записи"
      ],
      "metadata": {
        "id": "90IuVmuLwUDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подготовка выборки\n"
      ],
      "metadata": {
        "id": "IzbY6puEJWrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# qcut  - пример, как использовать собственную разбивку на кат. группы\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/FBosler/AdvancedPlotting/master/combined_set.csv')\n",
        "# присваиваем метки каждому году\n",
        "data['Mean Log GDP per capita']  = data.groupby('Year')['Log GDP per capita'].transform(\n",
        "    pd.qcut,\n",
        "    q=5,\n",
        "    labels=(['Lowest','Low','Medium','High','Highest'])"
      ],
      "metadata": {
        "id": "dhISJoR_inx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data= data.rename(columns={\"gkgk\": \"colu\"}) # - переименоватъ столбец\n",
        "data.rename(columns=dict(sector_name='region', region='locale'))\n",
        "data.rename(index={0: 'firstEntry', 1: 'secondEntry'}) # переименовать индекс\n",
        "data.rename_axis('inde', axis='rows') # переименовать название\n",
        "\n",
        "data.sample(5) # data.head(5) # data.tail(5)  # выбор выборки\n",
        "df.sample(frac = 0.00005) # доля выборки\n",
        "data.head(3) # - вывести первые 3 записи\n",
        "data.tail(1) # - вывести последнюю записъ\n",
        "data[1:4] # - срез. вывести топ значений фрейма\n",
        "\n",
        "df.drop_duplicates (subset=['team', 'points']) # удалить дубли в определенных столбцах -- df.drop_duplicates (keep= False ) для полной очистки от дублей\n",
        "\n",
        "######\n",
        "df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
        "Параметры:\n",
        "subset: Определяет столбцы DataFrame для проверки дубликатов. По умолчанию, используются все столбцы.\n",
        "keep: Определяет, какой из дубликатов оставлять:\n",
        "'first' - оставляет первое появление (по умолчанию)\n",
        "'last' - оставляет последнее появление\n",
        "False - удаляет все дубликаты\n",
        "inplace: определяет, изменять ли оригинальный DataFrame, или создать новый DataFrame (по умолчанию False)\n",
        "#####"
      ],
      "metadata": {
        "id": "XPn9UTZ3JaXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA"
      ],
      "metadata": {
        "id": "cCVIuDX7NBZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#найти ближайшие по похожести текста\n",
        "import fuzzywuzzy\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "t = df['SaleType'].unique()\n",
        "#df['0'].fillna(\"\", inplace=True)\n",
        "matches = fuzzywuzzy.process.extract(\"south korea\", t, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
        "matches\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# function to replace rows in the provided column of the provided dataframe\n",
        "# that match the provided string above the provided ratio with the provided string\n",
        "def replace_matches_in_column(df, column, string_to_match, min_ratio = 47):\n",
        "    # get a list of unique strings\n",
        "    strings = df[column].unique()\n",
        "    # get the top 10 closest matches to our input string\n",
        "    matches = fuzzywuzzy.process.extract(string_to_match, strings,\n",
        "                                        limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
        "    # only get matches with a ratio > 90\n",
        "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
        "    # get the rows of all the close matches in our dataframe\n",
        "    rows_with_matches = df[column].isin(close_matches)\n",
        "    # replace all rows with close matches with the input matches\n",
        "    df.loc[rows_with_matches, column] = string_to_match\n",
        "    # let us know the function's done\n",
        "    print(\"All done!\")\n",
        "Now that we have a function, we can put it to the test!\n",
        "# use the function we just wrote to replace close matches to \"south korea\" with \"south korea\"\n",
        "replace_matches_in_column(df=professors, column='Country', string_to_match=\"south korea\")\n"
      ],
      "metadata": {
        "id": "ZeyI0cb-FK9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to lower case\n",
        "professors['Country'] = professors['Country'].str.lower()\n",
        "# remove trailing white spaces детально разобраться для чего\n",
        "professors['Country'] = professors['Country'].str.strip()\n",
        "\n",
        "\n",
        "data.head()/.tail() # - первые/последние строки\n",
        "data.info() # - получение сведений о фрейме\n",
        "data.dtypes # типы данных столбцов\n",
        "data.describe # - статистическая информация о фрейме\n",
        "data.pole.value_counts() # - подсчет количества уникалных значений\n",
        "data.pole.rank(method='dense', ascending = False) # - ранжировать, можно добавить метод dense\n",
        "len(data) # - вывести количество строк во фрейме\n",
        "\n",
        "data.shape/size # - размерность фрейма\n",
        "print(\"Shape of the DataFrame:\", df.shape)\n",
        "print(\"Number of rows:\", df.shape[0])\n",
        "print(\"Number of columns:\", df.shape[1])\n",
        "\n",
        "\n",
        "df.dtypes.value_counts() # - получитъ распределение по типам данных\n",
        "data.isnull().mean() # пропуски во фрейме /df.notnull().mean() - противоположная функция / data.isna().mean() - тоже самое, что isnull\n",
        "len(data['pole'].unique()) # - количество уникалъных строк во фрейме\n",
        "data['pole'].unique() # вывод всех уникальных значений\n",
        "data.nunique() #  - количество уникальных записей по столбцам или строкам\n",
        "data.columns.tolist() # - получитъ наименования столбцов списком/ -- type(df.columns) - получиить типы данных\n",
        "data.nlargest(5, 'pole') #- вернет наибольшее число и Nsmallest() - вернет наименьшее,  nsmallest() и nlargest() - противоположные\n",
        "\n",
        "data.dropna() # - очистить данные таблицу от наловых значений\n",
        "df = df.dropna (subset=['assists']) # - очистить данные где есть null в конкретном столбце\n",
        "data.dropna (thresh=50) # - очистить ниже определенного порога\n",
        "df = df.reset_index(drop=True) # - сбросить индекс\n",
        "\n",
        "\n",
        "t[['lon', 'lat']] = t['Coordinate'].str.split(' ', expand=True) # разбить на 2 столбца\n",
        "t['lon'] = t['lon'].str.strip('(') # удалить символ в подстроке\n",
        "\n",
        "\n",
        "data.pole.shift(1) # -смещение поля на одно значение\n",
        "data= data[['pole1', 'pole2']] # - создание нового фрейма из подмножества\n",
        "df.insert(loc=0'''(позиция)''', column='id', value=new_column) #- вставить столбец в определенное место\n",
        "replace = data.replace(to_replace = 0 , value -9999) # - заменить значения во фрейме\n",
        "data.rename({'Pole_old':'Pole_new'}, axis = 1 )\n",
        "data_filled = data.fillna(0) # - замена всех наловых значений на 0\n",
        "data.drop(columns = ['pole1', 'pole2']) # - удлить столбцы\n",
        "data.drop_duplicates(keep = 'first') # - удалить дубликаты\n",
        "data.set_index(['age']) #-установить индекс\n",
        "\n",
        "######### пропуски в колонках#######################\n",
        "plt.figure(figsize=(25,10))\n",
        "#~df.isnull() вернет датафрейм, который содержит значения True для заполненных ячеек и False для пропущенных. Если бы мы написали df.isnull(), то получили бы датафрейм с логическим значением True для пропущенных значений и False для остальных значений.\n",
        "sns.heatmap(~df.isnull(), cbar=False, cmap=sns.cm.rocket_r).set_title('Темные области - заполненные данные, светлые - пропуски');"
      ],
      "metadata": {
        "id": "id5qUjEKND21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Количество пустых строк"
      ],
      "metadata": {
        "id": "vQCRsoSjFKKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_kc.isna().sum()[df_kc.isna().sum() > 0]\n",
        "[(n, df_kc[n].isnull().sum()) for n in df_kc.columns if df_kc[n].isnull().any()]\n",
        "tuple((n, df_kc[n].isnull().sum()) for n in df_kc.columns if df_kc[n].isnull().any())"
      ],
      "metadata": {
        "id": "10aZnqsGWwDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Фильтрация & сотрировка"
      ],
      "metadata": {
        "id": "Y97MqjL5N0Hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['поле'].isin(['F']) # вернет тру/фолс\n",
        "data['pole1'].where(data['pole1']>=25) # проверка структуры данных\n",
        "## метод фильтрации квери\n",
        "region = 'Center'\n",
        "quat = '2022-01-01'\n",
        "df_tall_named_sec = df_cor.query('Region == @region & Quarter == @quat')\n",
        "df_tall_named_sec\n",
        "#########################################################################################\n",
        "df.query(\"jobtitle.str.contains('captain', case=False)\")[['employeename',\t'basepay']] # case=False  аналог ловвер и аппер в sql\n",
        "\n",
        "data.filter(like='la', axis=1) # - выбрать столбцы в наименовании которых есть...\n",
        "data.drop(['colu'], axis=1).head() # - удаление столбца из фрейма данных\n",
        "data.append(data2.sum(axis=0), ignore_index=True) # - сумма по столбцу\n",
        "data[data['acell'].isin(['NN002998_I','NN002998_I'])] # -филътр\n",
        "data[data['acell'] == 'NN002998_I'] #(><) -филътр, для единственного значения\n",
        "data.sort_values('acell', ascending=True) # - сортировка фрейма\n",
        "df.pole.sort_values(ascending = False) # - второй вариант\n",
        "data.sort_values(by=['min', 'max'], ascending=True) # сортировка данных по нескольким столбцам\n",
        "\n",
        "data['sector_name'].fillna('NaN').value_counts().sort_values(ascending=False) # найти NaN\n",
        "df_filter = df ['ID']. isin (['A001', 'C022', ...])  # выбор строк с конкретными идентификаторами\n",
        "\n",
        "\n",
        "m_data = data[data['sector_name'].isnull()] # шаг 1. кол-во нулевых\n",
        "n_data = len(m_data) #  шаг 2. кол-во нулевых\n",
        "n_data = data.sector_name.isnull().sum() # v2 кол-во нулевых\n",
        "m_data = pd.isnull(data.sector_name).sum() # v3 кол-во нулевых\n",
        "\n",
        "\n",
        "#Параметр kind позволяет выбрать алгоритм сортировки: quicksort(Быстрая сортировка), mergesort (Сортировка слиянием) или heapsort (Пирамидальная сортировка).\n",
        "#По умолчанию используется алгоритм quicksort.\n",
        "df.sort_values('likes', ascending=False, kind=\"mergesort\")\n"
      ],
      "metadata": {
        "id": "z4K_ZOytN2Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Поиск по тексту"
      ],
      "metadata": {
        "id": "Gk4AbqDZcxwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[data[\"class\"].str.contains(\"Third\")]  # поиск подстроки\n",
        "\n",
        "###############Найти слово в df и сформировать таблицу с кол-вом слов ###############################################################\n",
        "bull = google_file_store['contents'].str.findall(' bull ').explode().count()\n",
        "bear = google_file_store['contents'].str.findall(' bear ').apply(len).sum()\n",
        "\n",
        "pd.DataFrame({\n",
        "    'word': ['bull', 'bear'],\n",
        "    'nentry': [bull,bear]})\n",
        "###############################################################\n",
        "\n"
      ],
      "metadata": {
        "id": "Ipqcuw24c0lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Типы данных"
      ],
      "metadata": {
        "id": "BpKLQ1aAOPzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#все ячейки в “Date” конвертируются в даты. Затем лишние строки удаляются с помощью dropna().\n",
        "df['Date'] =pd.to_datetime(df['Date'])\n",
        "df.dropna(subset = ['Date'], inplace = True)\n",
        "\n",
        "amazon_shipment['year-month'] = amazon_shipment['shipment_date'].dt.strftime('%Y-%m') # изменить формат вывода даты\n",
        "amazon_shipment['year-month'] = amazon_shipment['shipment_date'].apply(lambda x: pd.to_datetime(x).strftime('%Y-%m'))# изменить формат вывода даты, через лямбду\n",
        "\n",
        "no_2.resample(\"D\").mean().plot(style=\"-o\", figsize=(10, 5));\n",
        "\n",
        "df['new_month'] = pd.to_datetime(df['new_month']).dt.date # - перевести в дату\n",
        "data['col_new'] = data['col'].astype('float') # - изменить тип данных\n",
        "\n",
        "landslides['date_parsed'].dt.day\n",
        "\n",
        "# визуализировать дни\n",
        "sns.distplot(day_of_month_landslides, kde=False, bins=31)\n",
        "\n",
        "# посчитать количество дат разных форматов\n",
        "date_lengths = earthquakes.Date.str.len()\n",
        "date_lengths.value_counts()\n",
        "\n",
        "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format=\"%m/%d/%y\")\n",
        "landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True) угадать формат\n",
        "df['days'] = (pd.Timestamp('today') - df['date']).dt.days # разность между дат, для получения количества дней\n",
        "\n",
        "\n",
        "#изменение типов по отдельности\n",
        "earthquakes.loc[20650, \"Date\"] = \"03/13/2011\"\n",
        "earthquakes['date_parsed'] = pd.to_datetime(earthquakes['Date'], format=\"%m/%d/%Y\")"
      ],
      "metadata": {
        "id": "9vdfyRoZORlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объединение"
      ],
      "metadata": {
        "id": "kgNAQP2Wfkm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([A, B]) # по умолчанию axis = 0\n",
        "pd.concat([A, B], axis=1) # левый к правой\n",
        "pd.concat([df1, df2], ignore_index=True)  #- union all\n",
        "pd.concat([X, Y]).drop_duplicates(keep=False) #- union\n",
        "\n",
        "df = pd.merge(exp_new, diss_new[['Region', 'Category_fit']], left_index= True, right_index=True, how='left') # джойн по индексу\n",
        "df = pd.merge(left=df_a, left_on['A'], right=df_b, right_on=['B'], how='inner') #- merge (вывод новых столбцов)\n",
        "df = pd.merge(df1, df2, how='inner', left_on=['t1.region', 't1.acell'], right_on = ['Region','ACELL'], suffixes = ['_c','_o']) #- suffixes - добавит суфиксы для столбцов, если есть столбцы с одинаковым названием\n"
      ],
      "metadata": {
        "id": "cJULguArfnr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### пример ###\n",
        "data_set1 = {'Name': ['Rohit', 'Mohit', 'Sohit', 'Arun', 'Shubh'],\n",
        "             'Roll no': ['01', '02', '03', '04', '05'],\n",
        "             'maths': ['93', '63', '74', '94', '83'], 'science': ['88', '55', '66', '94', '35'], 'english': ['93', '74', '84', '92', '87']}\n",
        "data_set2 = {'Name': ['Karan', 'Rishu', 'Swetank', 'Rishabh', 'Shuvam'],\n",
        "             'Roll no': ['06', '07', '08', '09', '10'], 'maths': ['95', '62', '64', '14', '63'], 'science': ['58', '59', '86', '74', '55'], 'english': ['96', '77', '89', '42', '87']}\n",
        "# Changing the above dictionary into dataframe\n",
        "df1 = pd.DataFrame(data_set1)\n",
        "df2 = pd.DataFrame(data_set2)\n",
        "# Concating both the dataframes\n",
        "pd.concat([df1, df2], keys=['Set1', 'Set2'], ignore_index=True) #axis=1)"
      ],
      "metadata": {
        "id": "4_fNyPFA09Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Агрегация"
      ],
      "metadata": {
        "id": "pWuIAP3ZX3_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(['team'], as_index = False)['points'].nunique() # - группировка и посчет уникальных значений для одного столбца\n",
        "df.groupby(['team'], as_index = False)['points'].unique() # DataFrame.nunique пропускаются отсутствующие значения, поскольку параметр по умолчанию dropna=True/ Series.unique функция отсутствует не пропускает\n",
        "\n",
        "\n",
        "\n",
        "df.groupby(['pole1'], as_index=False).agg({ \"pole2\": pd.Series.nunique, \"pole3\": \"count\"}) #-группировка и посчет уникальных значений\n",
        "data.groupby('pole1').count() //mean() // median() // size() # - группировка по полю для всего df\n",
        "data.groupby('pole1').pole2.mean() # группировка по одному полю\n",
        "data.groupby('t1.acell')['t2.ne_id'].max().sort_index() # присваиваем индекс аселу и выводим макимальное значение из не_ид, сортируем от меньшего\n",
        "\n",
        "t.groupby(pd.Grouper(key='create_date_x', freq='M')).agg({\"msisdn\": \"count\",  \"nps_key\": \"mean\"}) # группирвка сразу до месяца в запросе\n",
        "\n",
        "data.groupby('report_date').report_date.size() #(count-разница?) вывести количество записей, аналог гроуп бай\n",
        "data.groupby(['t1.acell']).ne_id.agg([len, min, max]) # группировка с агрегацией и расчетом\n",
        "\n",
        "\n",
        "\n",
        "data[[\"sex\", \"pclass\"]].pivot_table(values=\"pclass\", columns=\"sex\", aggfunc=\"mean\") # pivot"
      ],
      "metadata": {
        "id": "UfXVohj3X6Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Функции"
      ],
      "metadata": {
        "id": "weWBNCVTP_n-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функции"
      ],
      "metadata": {
        "id": "cM5RAHo2sKh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.apply(lambda row: (row['pole1'],  row['pole2']), axis=1) # построчное применение функции\n",
        "level_map = {1: 'high', 2: 'medium', 3: 'low'} # преобразования данных\n",
        "df['c_level'] = df['c'].map(level_map) #\n",
        "###парс даты из произвольного формата###\n",
        "from datetime import datetime\n",
        "custom_datetime = lambda x: datetime.strptime(x, '%d%m%Y %H:%M:%S')\n",
        "df = pd.read_csv('тест2.csv', parse_dates=['date'], date_parser=custom_datetime)"
      ],
      "metadata": {
        "id": "dZZC9mjTsJ5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Пример: добавить новый столбец на основе условий\n",
        "def new_stat (row):\n",
        "  if 'Есть' in row['Вариант1']:\n",
        "    return 'Свой'\n",
        "  elif ...\n",
        "  else:\n",
        "    ('')\n",
        "df_full['flag']=df_full.apply(new_stat, axis = 1)"
      ],
      "metadata": {
        "id": "F82UeyBVQFxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Операции над значениями"
      ],
      "metadata": {
        "id": "cnCYhuytzgMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['pole1'].round(2) # округлить\n",
        "df['pole1'].nunique() # посчитать уникальнных\n",
        "df1[\"pole2\"].cumsum() # накопительный итог\n",
        "\n",
        "\n",
        "data[\"sex\"].replace({\"male\": \"m\", \"female\": \"f\"}) # изменить значения в подстроке"
      ],
      "metadata": {
        "id": "dBRI1Fn3zgZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loc iloc"
      ],
      "metadata": {
        "id": "Ssw6Wxx4ecFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.iloc[:0]  # вывести шапку\n",
        "data.iloc[552] # вывод СПИСОК строки по номеру\n",
        "data.iloc[552] # вывод строки по индексу\n",
        "data.iloc[0:5] # вывести первые 5 значений, с 0 до 4\n",
        "data.loc[0:5]  # вывести первые 6 значений, с 0 до 5\n",
        "data.recdate  #data['recdate'] #data.iloc[:,0]#  вывод столбца в формате списка\n",
        "data.iloc[2,1] # data.loc[2,'ACELL'] вывести 3 строку из 2-ого столбца\n",
        "data.iloc[:3,1] #data.iloc[0:3,1]#data.iloc[[0,1,2],1]  вывести количество строк из столбца\n",
        "data.iloc[-3:,1] #  вывести второе значение снизу, второго столбца\n",
        "data.iloc[:, 0:2] # вывести всю информацию по двум столбцам\n",
        "data.iloc[3, [1, 2, 3]] # список третьей строки по первым 3м столбцам\n",
        "data.loc[1:5, 'ACELL': 'Region'] # вывод таблицы с первой по пятаю, 2 столбца\n",
        "data.iloc[[0]] # вывести первую строку\n",
        "data.loc[:,['recdate','ACELL','Region','flag']] # print all\n",
        "data.loc[data.flag.isin(['100', '50'])] # филътр несколъко условий\n",
        "data.loc[data.flag.isnull()]\n",
        "data.loc[data.flag.notnull()]\n",
        "data[data['t2.ne_id'].isnull()] # вариация поиска нулевых значений\n",
        "############## Примеры фильтров выводом фрейма\n",
        "data.loc[data['ACELL'] == 'NN000231_A'] # вывести отфильтрованную таблицу\n",
        "data.loc[data.ACELL == 'NN000231_A'] # то же что и предыдущее\n",
        "data.loc[data.flag == '0'] # филътр на таблицу с выводом всего датафрейма\n",
        "data.loc[(data.flag == '0') & (data.recdate =='2022-02-28 23:00')] # склеенный фильтр на таблицу\n",
        "data.loc[(data['ACELL'] == 'NN000231_A' ) & (data['recdate'] == '2022-02-20 16:00'), 'recdate':'flag'] # пример склеенного фильтра\n",
        "###### replace ######\n",
        "data.loc[data['ACELL'] == 'NN000231_A'] = 'BIG'\n",
        "data.loc[data['ACELL'] == 'NN000231_A', 'ACELL':'Mean_flag_integrity 4G (%)'].head() # пример реплэйса с фильтром и выводом"
      ],
      "metadata": {
        "id": "LJPO2X0Sej8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Визуализация"
      ],
      "metadata": {
        "id": "ICTxlRpNB0nK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "resample"
      ],
      "metadata": {
        "id": "k5U-_iIzB1da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_set1.set_index('pickup').resample(\"D\").median().plot(style=\"-o\", figsize=(10, 5)) #установить индекс с ресамплом построить ряд\n",
        "#Resample нужен для перерасчета или изменения шага времени данных, можно изменять частоту временных данных и изменять их размер\n",
        "#Создаем временной ряд с данными о продажах ежедневно\n",
        "#Изменяем размер данных: ежедневные данные на среднемесячные данные  ---   monthly_temp = daily_temp.resample('M').mean()"
      ],
      "metadata": {
        "id": "inwX0jqyB1si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Итерации"
      ],
      "metadata": {
        "id": "Q22IUs73wxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Иттерации по фрейму"
      ],
      "metadata": {
        "id": "pJ9e_kWr9n7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## создать тестовые данные\n",
        "df = pd.DataFrame([[1, \"A\"], [2, \"B\"], [3, \"C\"]], columns = [\"col1\", \"col2\"])\n",
        "print(df)\n",
        "# итерация по тестовым данным\n",
        "print(\"Method 1:\", end = \" \")\n",
        "for index in range(len(df)):\n",
        "    print(df[\"col1\"][index], end = \" \")\n",
        "\n",
        "print(\"\\nMethod 2:\", end = \" \")\n",
        "for index, row in df.iterrows():\n",
        "    print(row[\"col1\"], end = \" \")\n",
        "\n",
        "print(\"\\nMethod 3:\", end = \" \")\n",
        "for row in df.itertuples():\n",
        "    print(row.col1, end = \" \")"
      ],
      "metadata": {
        "id": "ni33tQry8a1p",
        "outputId": "edef5739-ab80-4a37-f194-d3bba73ead90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   col1 col2\n",
            "0     1    A\n",
            "1     2    B\n",
            "2     3    C\n",
            "Method 1: 1 2 3 \n",
            "Method 2: 1 2 3 \n",
            "Method 3: 1 2 3 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# итерация с примером\n",
        "data=data.rename(columns={\"class\": \"class_\"})\n",
        "f = 0\n",
        "for i in data.itertuples():\n",
        "    if i.class_ ==  'Third':\n",
        "      f += 1\n",
        "print(f)\n",
        "f = 0\n",
        "for i, row in data.iterrows():\n",
        "  if row['class_'] == 'Third':\n",
        "    f += 1\n",
        "print(f)\n",
        "\n",
        "f = 0\n",
        "for i in range(len(data)):\n",
        "  if data['class_'][i] == 'Third':\n",
        "    f += 1\n",
        "print(f)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pSsQjgI_2BT",
        "outputId": "7af460d6-51bc-4a30-a58c-422686819340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "491\n",
            "491\n",
            "491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Примеры"
      ],
      "metadata": {
        "id": "20AzRySy8774"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pivot\n",
        "t = pd.merge(ms_user_dimension, ms_acc_dimension, on = 'acc_id', how = 'left')\n",
        "df = pd.merge(t, ms_download_facts, on = 'user_id', how = 'left')\n",
        "\n",
        "df = df.pivot_table(index='date', columns='paying_customer', values='downloads', aggfunc='sum')\n",
        "df[df['no']>df['yes']].reset_index()\n"
      ],
      "metadata": {
        "id": "POnG1k0Ew8B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#аналог кейса\n",
        "df = sf_restaurant_health_violations.copy()\n",
        "df['business_type'] = np.where(df['business_name'].str.contains('restaurant', case = False ),'restaurant',\n",
        "    np.where(df['business_name'].str.contains('cafe|café|coffee', case = False ),'cafe',\n",
        "    np.where(df['business_name'].str.contains('school', case = False ),'school', 'other')))\n",
        "\n",
        "result = df[['business_name','business_type']].drop_duplicates()"
      ],
      "metadata": {
        "id": "2Z_3OKFtoce4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "В Python существует несколько способов объединения (конкатенации) нескольких DataFrame в один:\n",
        "\n",
        "Метод concat из библиотеки pandas. Данный метод позволяет конкатенировать несколько DataFrame по вертикали или по горизонтали.\n",
        "Например, чтобы объединить два DataFrame 'df1' и 'df2' по вертикали, можно использовать следующий код:\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "new_df = pd.concat([df1, df2], axis = 0, ignore_index=True)\n",
        "Здесь указан параметр axis = 0, который указывает, что конкатенация должна быть выполнена по вертикали (т.е. строки из 'df2' добавляются внизу строк из 'df1'). Параметр ignore_index=True используется для того, чтобы перенумеровать строки в результирующем DataFrame.\n",
        "\n",
        "Оператор '+'. Данный оператор можно использовать для объединения двух DataFrame по вертикали, но оба DataFrame должны иметь одинаковый набор столбцов (порядок столбцов не важен).\n",
        "Например:\n",
        "\n",
        "new_df = df1 + df2\n",
        "Здесь 'df1' и 'df2' должны иметь одинаковый набор столбцов, а оператор '+' выполняет поэлементное сложение по всем строкам с одинаковыми индексами.\n",
        "\n",
        "Метод append из библиотеки pandas. Этот метод делает примерно тоже самое, что и метод concat, но может быть более удобен для объединения небольших DataFrame.\n",
        "Например:\n",
        "\n",
        "new_df = df1.append(df2, ignore_index=True)\n",
        "Здесь 'df1' и 'df2' объединяются по вертикали, а параметр ignore_index=True используется для перенумерации строк в результирующем DataFrame."
      ],
      "metadata": {
        "id": "TIqSGllC0O2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# максимальные по группам\n",
        "t = employee.groupby(['department'], as_index = False)['salary'].max()\n",
        "pd.merge(employee, t ,  on = ['department', 'salary'], how = 'inner')[['department',\t'first_name',\t'salary']]\n",
        "\n",
        "df = employee[['first_name', 'department', 'salary']]\n",
        "df['rank_in_salary'] = df.groupby('department')['salary'].rank(method = 'min', ascending = False)\n",
        "top = df.loc[df['rank_in_salary'] == 1][['department','first_name', 'salary']]\n",
        "\n",
        "df = employee[employee['salary'] ==\n",
        "    employee.groupby('department')['salary'].transform('max')]\n",
        "df[['department', 'first_name', 'salary']]"
      ],
      "metadata": {
        "id": "nOn-TkHZjpS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#поиск по подстроке\n",
        "des = r'\\b(plum|cherry|rose|hazelnut)\\b'\n",
        "winemag_p1.query('description.str.contains(@des, case=False)')['winery'] #case=False без учета регистра"
      ],
      "metadata": {
        "id": "fDqb5LIPZNLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#разбить строку на подстроки\n",
        "yelp_business['cat'] = yelp_business['categories'].str.split(\";\")\n",
        "yelp_business.explode('cat').groupby(['cat'], as_index = False)['review_count'].sum().sort_values(by = 'review_count', ascending = False)"
      ],
      "metadata": {
        "id": "2Jd_Vhk0T_XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#max find\n",
        "salesforce_employees.query('manager_id==13' ).groupby(['first_name'],as_index = False)['target'].max().sort_values('target', ascending = False).head(3)\n",
        "salesforce_employees[salesforce_employees.manager_id==13].nlargest(1, 'target', keep='all')[['first_name', 'target']]"
      ],
      "metadata": {
        "id": "F87cNO9yGuBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# фильтр по дате\n",
        "orders.loc[orders.order_date.dt.to_period('M') == '2019-03', ['cust_id', 'total_order_cost']].groupby('cust_id', as_index=False).sum().sort_values('total_order_cost', ascending=False)"
      ],
      "metadata": {
        "id": "kI5nsNqkDke4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "forbes_global_2010_2014.nlargest(3, ['profits'])[['company', 'profits']] # максимальное знаение\n",
        "worker[worker.salary == worker.salary.max()].merge(title, how = 'inner', left_on = 'worker_id', right_on = 'worker_ref_id')[['worker_title']]\n",
        "\n",
        "yelp_reviews[['business_name', 'review_text']][yelp_reviews['cool'] == yelp_reviews['cool'].max()] # найти максимум"
      ],
      "metadata": {
        "id": "NWU7SFbB6iMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####найти среднее время проведенное на сайте\n",
        "pl = facebook_web_log.query('action == \"page_load\"')\n",
        "pl['day']=pl['timestamp'].dt.date\n",
        "pl = pl.groupby(['user_id',\t 'day'], as_index = False).max()\n",
        "\n",
        "pe = facebook_web_log.query('action == \"page_exit\"')\n",
        "pe['day']=pe['timestamp'].dt.date\n",
        "pe = pe.groupby(['user_id',\t 'day'], as_index = False).min()\n",
        "\n",
        "df = pd.merge(pl, pe, on = ['user_id', 'day'], how = 'inner')\n",
        "df['diff'] = df['timestamp_y'] - df['timestamp_x']\n",
        "df.groupby(['user_id'], as_index = False)['diff'].apply(np.mean) # если ошибка данных, можно применить апплу с нампи"
      ],
      "metadata": {
        "id": "63eC7biD6l6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F3zJUsNZ62Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQL Lead и Lag на питоне"
      ],
      "metadata": {
        "id": "niC2ScyqUskz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# аналог row_number()\n",
        "# Создаем DataFrame\n",
        "df = pd.DataFrame({'group': ['A', 'A', 'B', 'B', 'B'], 'value': [10, 20, 30, 40, 50]})\n",
        "# Пронумеровать строки в каждой группе\n",
        "df['row_number'] = df.groupby('group').cumcount() + 1\n",
        "# Пронумеровать все строки\n",
        "df['row_number'] = list(range(1, len(df) + 1))"
      ],
      "metadata": {
        "id": "V_ONH6L5bjPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qR0zpq3zbjRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h0RWCUdPbjTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start writing code\n",
        "sf_transactions['cr_dt'] = sf_transactions.created_at.dt.strftime('%Y-%m')\n",
        "t = sf_transactions.groupby(['cr_dt'], as_index = False)['value'].sum()\n",
        "\n",
        "t['lag'] = t['value'].shift(1)\n",
        "t['revenue_diff_pct'] = round(((t['value'] - t['lag'])/ t['lag'])*100, 2)\n",
        "t[['cr_dt', 'revenue_diff_pct']]\n",
        "\n",
        "\n",
        "# v2\n",
        "sf_transactions.head()\n",
        "res = sf_transactions.set_index('created_at')['value'].resample('M').sum().pct_change()*100\n",
        "res.round(2).to_period().reset_index()"
      ],
      "metadata": {
        "id": "KBF0suAguuiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['lag'] = df.groupby('user_id')['created_at'].shift(1) # lag с партицией"
      ],
      "metadata": {
        "id": "ZHCJfQOK8918"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'DATE': [1, 2, 3, 4, 5],\n",
        "                   'VOLUME': [100, 200, 300,400,500],\n",
        "                   'PRICE': [214, 234, 253,272,291]})\n",
        "#Мы можем легко вычислить среднюю цену акций за три последних дня и создать новый столбец, как показано ниже:\n",
        "df['LAST_3_DAYS_AVE_PRICE'] = (df['PRICE'].shift(1,fill_value=0) +\n",
        "                               df['PRICE'].shift(2,fill_value=0) +\n",
        "                               df['PRICE'].shift(3,fill_value=0))/3\n",
        "\n",
        "#Можно пойти дальше и получить значение из следующего временного интервала или ряда:\n",
        "df['TOMORROW_PRICE'] = df['PRICE'].shift(-1,fill_value=0)"
      ],
      "metadata": {
        "id": "bVBAD-5IUvGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Добавление нового столбца в заданном месте датафрейма\n",
        "df['Population density'] = df['City Population']/df['City Area']\n",
        "df.insert(loc=3, column='Population density', value=(df['City Population']/df['City Area']))"
      ],
      "metadata": {
        "id": "yPpqkp-uVRZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#для нахождения уникальных значений:\n",
        "b = pd.Series(['ab','bc','cd',1,'cd','cd','bc','ab','bc',1,2,3,2,3,np.nan,1,np.nan])\n",
        "b.value_counts()"
      ],
      "metadata": {
        "id": "4GNvAJdHVRhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Выбор столбца на основе типа данных\n",
        "df.select_dtypes(exclude=['int64','float64'])\n",
        "df.select_dtypes(include='number',exclude='float64')"
      ],
      "metadata": {
        "id": "rbNiwxM3Yiwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mask() для условия if-else #Обратимся к датафрейму, в котором нужно изменить знак всех элементов, кратных двум без остатка.\n",
        "df = pd.DataFrame(np.arange(15).reshape(-1, 3), columns=['A', 'B','C'])\n",
        "print(df)\n",
        "#С помощью mask проверяем делится ли элемент на 2 без остатка.\n",
        "#При соотвествии условию меняем знак элемента\n",
        "df.mask(df % 2 == 0,-df)"
      ],
      "metadata": {
        "id": "pNKGXws9Y7WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fJ2JT-3095En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYjoo6FZzUnL"
      },
      "outputs": [],
      "source": [
        "###################   Other...\n",
        "df = data.append(new_row, ignore_index=True) # добавляет элемент в конец списка\n",
        "df = data.drop(['t1.macroregion'], axis = 1) # удалить столбец\n",
        "df_copied = df.copy() # поверхностное и глубокое копирование\n",
        "df['vol_new'] = round(df['ut_driver']) # округлить до выделенного значения\n",
        "\n",
        "# Перебор строк во фрейме()\n",
        "for idx,row in df2[:1].iterrows():\n",
        "    print(idx, row)\n",
        "#%%\n",
        "\n",
        "data ['acell'].tolist() # - перевести в список стлбец\n",
        "data['colu'] = True # - добавитъ столбец во фрейм данных\n",
        "\n",
        "###кейс с фильтрацией:\n",
        "data['t1.region'].unique() #  вывести уникальные значения фрейма\n",
        "data = data.set_index('t1.region') # присвоить индекс\n",
        "data.drop(['RYAZAN', 'TVER'],axis = 0) # установить фильтрацию по столбцу и дропнуть эти данные во фрейме"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2lb4Xhwjli4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' alt+shift+A ''' - закомитить строку\n",
        "#ctrl+/ - закомитить строку"
      ],
      "metadata": {
        "id": "iBCfxtGoMMkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Генерация данных\n",
        "import random\n",
        "print(*[random.randint(1, 12) for i in range(12)])\n",
        "\n",
        "lst = [int(i) for i in range(1, 13)]\n",
        "random.shuffle(lst)\n",
        "print(*lst)\n",
        "\n",
        "\n",
        "for _ in range(12):\n",
        "   num = random.randint(1, 12)\n",
        "   print(num)"
      ],
      "metadata": {
        "id": "D60236z1EkvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создадим DataFrame\n",
        "df1 = pd.DataFrame({'name': ['Alice', 'Bob', 'Charlie'], 'age': [25, 30, 35], 'salary': [50000, 60000, 70000]})"
      ],
      "metadata": {
        "id": "ZTB6lR79FBN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#heatmap - без половины\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Создать случайную матрицу корреляционных коэффициентов\n",
        "corr_mat = np.random.rand(10, 10)\n",
        "# Получить верхний треугольник корреляционной матрицы\n",
        "mask = np.triu(np.ones_like(corr_mat, dtype=bool))\n",
        "# Отобразить heatmap с верхним треугольником\n",
        "sns.heatmap(corr_mat, mask=mask)\n",
        "# Показать график\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ixi23znN3kxb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}